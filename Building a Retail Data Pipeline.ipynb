{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bedc2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create the transform() function with one parameter: \"raw_data\"\n",
    "def transform(raw_data):\n",
    "    #filling the missing data\n",
    "    total_missing = raw_data.isna().sum(axis=0)\n",
    "    misssing_value_columns= list(total_missing[total_missing > 0].index)\n",
    "    # Write your code here\n",
    "    for missing in misssing_value_columns:\n",
    "        raw_data[missing] = raw_data[missing].fillna(raw_data[missing].mode()[0])\n",
    "\n",
    "    # convert Date series to datime format \"%Y-%m-%d\"\n",
    "    # format and extract month in 'Date' field\n",
    "    raw_data['Date'] = pd.to_datetime(raw_data['Date'], format=\"%Y-%m-%d\")\n",
    "    raw_data['Month'] = raw_data['Date'].dt.month.astype('int32')\n",
    "\n",
    "    # keeping the rows where the weekly sales are over $10,000 and dropping the unnecessary columns\n",
    "    raw_data = raw_data[raw_data['Weekly_Sales'] > 10000]\n",
    "\n",
    "    # list of the necessary columns\n",
    "    nec_columns = ['Store_ID', 'Weekly_Sales', 'IsHoliday', 'Month', 'CPI', 'Unemployment']\n",
    "    # list all column\n",
    "    columns = list(raw_data.columns)\n",
    "    # drop unnecessary columns\n",
    "    for column in columns:\n",
    "        if column not in nec_columns:\n",
    "            raw_data = raw_data.drop(column, axis=1)\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9357a734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  index  Store_ID        Date  Dept  Weekly_Sales  IsHoliday  \\\n",
      "0           0      0         1  2010-02-05     1      24924.50          0   \n",
      "1           1      1         1  2010-02-05    26      11737.12          0   \n",
      "2           2      2         1  2010-02-05    17      13223.76          0   \n",
      "3           3      3         1  2010-02-05    45         37.44          0   \n",
      "4           4      4         1  2010-02-05    28       1085.29          0   \n",
      "\n",
      "   Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  \\\n",
      "0        42.31       2.572        0.0        0.0        0.0        0.0   \n",
      "1        42.31       2.572        0.0        0.0        0.0        0.0   \n",
      "2        42.31       2.572        0.0        0.0        0.0        0.0   \n",
      "3        42.31       2.572        0.0        0.0        0.0        0.0   \n",
      "4        42.31       2.572        0.0        0.0        0.0        0.0   \n",
      "\n",
      "   MarkDown5         CPI  Unemployment  Type      Size  \n",
      "0        0.0  211.096358         8.106   3.0  151315.0  \n",
      "1        0.0  211.096358         8.106   3.0  151315.0  \n",
      "2        0.0  211.096358         8.106   3.0  151315.0  \n",
      "3        0.0  211.096358           NaN   3.0  151315.0  \n",
      "4        0.0  211.096358           NaN   3.0  151315.0  \n"
     ]
    }
   ],
   "source": [
    "grocery_sales = pd.read_csv(\"grocery_sales.csv\")\n",
    "print(grocery_sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f528e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the transform() function and pass the merged DataFrame\n",
    "clean_data = transform(grocery_sales)\n",
    "\n",
    "# check whether there is any missing data.\n",
    "assert clean_data.isna().any().any() == False\n",
    "# check Values of Weekly sale > 10000$\n",
    "assert (clean_data['Weekly_Sales']>10000).any()\n",
    "# check added 'Month' field\n",
    "assert 'Month' in list(clean_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0a86184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\n",
    "def avg_weekly_sales_per_month(clean_data):\n",
    "    mw = clean_data[['Month', 'Weekly_Sales']]\n",
    "    Avg_Sales = mw.groupby('Month')['Weekly_Sales'].mean()\n",
    "    agg = Avg_Sales.round(2)\n",
    "    agg = agg.reset_index()\n",
    "    agg.rename(columns={\"Weekly_Sales\": \"Avg_Sales\"}, inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b36ae540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Month  Avg_Sales\n",
      "0       1   33174.18\n",
      "1       2   34331.29\n",
      "2       3   33227.31\n",
      "3       4   33414.78\n",
      "4       5   33339.89\n",
      "5       6   34582.47\n",
      "6       7   33930.77\n",
      "7       8   33644.79\n",
      "8       9   33266.59\n",
      "9      10   32736.99\n",
      "10     11   36594.03\n",
      "11     12   39248.98\n"
     ]
    }
   ],
   "source": [
    "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\n",
    "agg_data = avg_weekly_sales_per_month(clean_data)\n",
    "print(agg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63df9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\n",
    "def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n",
    "    full_data.to_csv(full_data_file_path, index=False)\n",
    "    agg_data.to_csv(agg_data_file_path, index=False)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37d74b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths\n",
    "load(clean_data, \"clean_data.csv\", agg_data, \"agg_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ead22953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\n",
    "def validation(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise Exception(f\"File not found: {file_path}\")\n",
    "\n",
    "    print(\"File exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "267e483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n",
      "File exists!\n"
     ]
    }
   ],
   "source": [
    "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\n",
    "validation(\"clean_data.csv\")\n",
    "validation(\"agg_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
